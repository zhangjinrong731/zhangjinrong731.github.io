<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello, My name is Jinrong Zhang</title>
    <url>/2025/03/25/hello-world/</url>
    <content><![CDATA[<h1 id="helloits-a-happy-day">👋 Hello，it's a happy day~~~</h1>
<ul>
<li>My name is <strong>Jinrong Zhang</strong>, and I am a researcher specializing in computer vision and multimodal large models. 🚀</li>
<li>If you have any interest in collaboration or academic exchange, please feel free to contact me.</li>
</ul>
<hr>
<h3 id="about-me">🧑‍💻 About Me</h3>
<p>📚 <strong>PhD Student</strong> in Electronic Information at Harbin Institute of Technology, Shenzhen.<br>
🔬 <strong>Research Interests:</strong></p>
<ul>
<li>Video Understanding and Generation</li>
<li>Multimodal Representation</li>
<li>Temporal Action Segmentation</li>
</ul>
<hr>
<h3 id="research-papers">📄 Research Papers</h3>
<p>I love publishing and sharing my findings with the world! Here's a list of some of my published research papers:</p>
<ol type="1">
<li><p><strong>Just a Few Glances: Open-Set Visual Perception with Image Prompt Paradigm</strong> – AAAI, CCF-A, 2025</p>
<p>🔗 <a href="https://arxiv.org/abs/2412.10719">Link to orginal paper</a></p></li>
<li><p><strong>End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning</strong> – TNNLS, CCF-B, IF=10.2, 2025</p>
<p>🔗 <a href="https://arxiv.org/abs/2309.15683">Link to orginal paper</a></p></li>
<li><p><strong>Flexible Streaming Temporal Action Segmentation with Diffusion Models</strong> – ICME, CCF-B, 2025</p>
<p>🔗 <a href="">Accepted (to be indexed soon)</a></p></li>
<li><p><strong>DTOS: Dynamic Time Object Sensing with Large Multimodal Model</strong> – CVPR, CCF-A, 2025</p>
<p>🔗 <a href="">Accepted (to be indexed soon)</a></p></li>
<li><p><strong>Cluster-Refined Optimal Transport for Unsupervised Action Segmentation</strong> – ICASSP, CCF-B, 2025</p>
<p>🔗 <a href="https://ieeexplore.ieee.org/abstract/document/10887693">Link to orginal paper</a></p></li>
</ol>
<p>On the Papers page, you can also access the key details of these research papers.</p>
<hr>
<h3 id="internship-experience">💼 Internship Experience</h3>
<ul>
<li><strong>Xiaomi AI Lab</strong> – <strong>AI Research Intern</strong><br>
<em>2024/2 – 2025/10</em>
<ul>
<li>I provided a large model solution for access permission detection at the Xiaomi car factory and successfully implemented it.</li>
<li>During my internship, I published a paper in AAAI.</li>
</ul></li>
</ul>
<hr>
<h3 id="my-profile">🌐 My Profile</h3>
<ul>
<li>🌍 Website: <a href="https://scholar.google.com/citations?user=doMwcRYAAAAJ&amp;hl=en">Google Shcoral</a></li>
</ul>
<hr>
]]></content>
  </entry>
  <entry>
    <title>Flexible Streaming Temporal Action Segmentation with Diffusion Models</title>
    <url>/2025/03/25/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/</url>
    <content><![CDATA[<h1 id="flexible-streaming-temporal-action-segmentation-with-diffusion-models">Flexible Streaming Temporal Action Segmentation with Diffusion Models</h1>
<p><strong>Jinrong Zhang</strong><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="0.898ex" height="1.949ex" role="img" focusable="false" viewBox="0 -861.5 397 861.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><path data-c="2020" d="M182 675Q195 705 222 705Q234 705 243 700T253 691T263 675L262 655Q262 620 252 549T240 454V449Q250 451 288 461T346 472T377 461T389 431Q389 417 379 404T346 390Q327 390 288 401T243 412H240V405Q245 367 250 339T258 301T261 274T263 225Q263 124 255 -41T239 -213Q236 -216 222 -216H217Q206 -216 204 -212T200 -186Q199 -175 199 -168Q181 38 181 225Q181 265 182 280T191 327T204 405V412H201Q196 412 157 401T98 390Q76 390 66 403T55 431T65 458T98 472Q116 472 155 462T205 449Q204 452 204 460T201 490T193 547Q182 619 182 655V675Z"></path></g></g></g></g></svg></mjx-container></span>, Wenjun Wen<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="0.898ex" height="1.949ex" role="img" focusable="false" viewBox="0 -861.5 397 861.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><path data-c="2020" d="M182 675Q195 705 222 705Q234 705 243 700T253 691T263 675L262 655Q262 620 252 549T240 454V449Q250 451 288 461T346 472T377 461T389 431Q389 417 379 404T346 390Q327 390 288 401T243 412H240V405Q245 367 250 339T258 301T261 274T263 225Q263 124 255 -41T239 -213Q236 -216 222 -216H217Q206 -216 204 -212T200 -186Q199 -175 199 -168Q181 38 181 225Q181 265 182 280T191 327T204 405V412H201Q196 412 157 401T98 390Q76 390 66 403T55 431T65 458T98 472Q116 472 155 462T205 449Q204 452 204 460T201 490T193 547Q182 619 182 655V675Z"></path></g></g></g></g></svg></mjx-container></span>, Shenglan Liu, Sifan Zhang, Yuning Ding and Lin Feng 🔗 <a href="">Accepted by ICME25 (to be indexed soon)</a></p>
<h2 id="abstract">Abstract</h2>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.051ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 2232.6 665"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(743.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1799.6,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></svg></mjx-container></span></p>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.291ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 2338.6 665"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(849.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1905.6,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></svg></mjx-container></span>Temporal distribution shifts occur not only in low-dimensional time-series data but also in high-dimensional data like videos. This phenomenon leads to significant performance degeneration in video understanding methods such as streaming temporal action segmentation. To address this issue, we propose a flexible streaming temporal action segmentation model with diffusion models (FSTAS-DM). By utilizing streaming video clips with varying feature distributions as control conditions, our model can adapt to the shifts and inconsistency of the distribution between the training and testing domains. Additionally, we have introduced a multi-stage conditional control training strategy (MSCC), which enhances the temporal generalization ability of the model. Our method demonstrates commendable performance on datasets like GTEA, 50Salads, and Breakfast.</p>
<span id="more"></span>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models\main_fig2.jpg" width="500" alt="图片说明" align="center"></p>
<h2 id="introduction">Introduction</h2>
<p>Temporal distribution shifts (TDS) occur widely in the field of times series, refering to the distribution shifts between training and testing domains. This inconsistency hinders achieving optimal performance. Most TDS studies focus on low-dimensional data, such as finance, weather, and electricity usage. However, TDS also occurs in high-dimensional data like videos. To facilitate training, video understanding models are always trained by a fixed number of frames that sample from videos in the whole training process. Those models perform well when tested with the same number of frames, but performance degradation occurred once the number of frames changed. We called this phenomenon video temporal distribution shifts (VTDS) because there are shifts and inconsistencies in video feature distributions between training and testing domains, as shown in Figure. VTDS severely limits the temporal generalization capability of video understanding models, which refers to accurately and stably mapping feature data with previously unseen distributions from feature space to label space. If a model is trained on video clips of the same length, it tends to be trained only on samples with a similar data distribution, significantly limiting the model's application scope.</p>
<p>Streaming temporal action segmentation (STAS) task, which decomposes a full video into a stream of video clips and classifies each frame of every clip in temporal order, is a task significantly affected by VTDS. In practical scenarios, different capture devices provide streaming video clips of varying lengths, and different applications have varying requirements. However, for ease of engineering implementation, current models use the same clip length for both training and testing. When the length of video clips used in testing does not align with the training, the performance of models is substantially decreased. It is VTDS that is consistent with the phenomenon in the STAS task.</p>
<p>To mitigate VTDS, we introduce the diffusion structure into STAS, inspired by DiffAct. Diffusion models, exceling in learning complex and diverse data distributions, are widely used in image generation, style transfer, and time-series tasks. Inspired by this, we propose FSTAS-DM, a flexible streaming temporal action segmentation model with diffusion models. Our model generates predicted labels from noisy data with RGB videos as control inputs. The diffusion structure enables learning diverse distributions and adapting to distribution shifts in the testing domain. We further enhance generalization with a multi-stage conditional control strategy (MSCC), dividing training into stages. In each stage, streaming clips with different feature distributions guide the diffusion process. This stage-by-stage learning helps denoise sequences into final temporal label predictions.</p>
<p>In summary, this paper presents three main contributions: (1) We discover the phenomenon of VTDS in the video understanding field and tackle it by proposing FSTAS-DM. (2) The multi-stage conditional control training strategy we proposed can enhance the temporal generalization ability and significantly reduce the training cost when STAS models are applied to different video clip lengths. (3) The proposed method achieves commendable performance on datatsets like GTEA, 50Salads, and Breakfast.</p>
<h2 id="method">Method</h2>
<p>FSTAS-DM consists of a streaming video conditional control process and an inference process with conditional control, as shown in Figure. During the streaming video conditional control process, conditional latent variables are obtained through a flexible prompt network under different conditional information. Streaming video clips with unique data distribution, namely, which have different clip lengths, are used as diverse conditional information. In the inference process with conditional control, a conditional control denoising model (CCDM), guided by the conditional latent variables, denoises the original Gaussian noise, to produce the next stage result. Iteratively applying this inference process ultimately yields the predicted segmentation result of the streaming video clip.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models\Model5.jpg" alt="图片说明" align="center"></p>
<h2 id="experiments">Experiments</h2>
<h3 id="the-impact-of-video-temporal-distribution-shifts">The Impact of Video Temporal Distribution Shifts</h3>
<p>Figure illustrates the significant impact of VTDS on model performance. We adopt three typical temporal action segmentation models with entirely different structures to STAS, including MSTCN, ASFormer, and DiffAct. We train them on video clip streams with a fixed length of 128 and test them across a variety of clip lengths. It is apparent that VTDS leads to a severe training-testing discrepancy, which means model performance deteriorates as the difference in video clip lengths between training and testing increases. Such performance degradation highlights the inadequate temporal generalization capability of existing temporal action segmentation methods, showing their difficulty in adapting to the altered data distribution caused by VTDS.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/Acc_F054.jpg" alt="图片说明" width="500" align="center"></p>
<p>We also compare FSTAS-DM with these three methods. It is evident that FSTAS-DM not only significantly outperforms these three methods in STAS but also greatly mitigates the impact of VTDS.</p>
<h3 id="the-temporal-generalization-capability-of-fstas-dm">The Temporal Generalization Capability of FSTAS-DM</h3>
<p>Figure shows FSTAS-DM's temporal generalization capability. Trained and tested on streaming clips of varying lengths, it outperformed fixed-length models, which struggled with shorter clips. Using MSCC with clip lengths of 32, 64, and 128 improved performance across all lengths. The results demonstrate that FSTAS-DM captures VTDS's impact on data distribution by learning from limited samples.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/flexible_gtea7.jpg" alt="图片说明" align="center"></p>
<p>As mentioned, we introduce the diffusion structure into STAS to enhance temporal generalization. Figure  shows its impact on streaming video features. STAS aims to map streaming video clips from an undefined data space to a relatively stable label space. To analyze the effect of diffusion on VTDS, we removed the diffusion structure from FSTAS-DM, creating FSTAS. Figure compares the cosine similarity matrices of prediction features for FSTAS-DM and FSTAS. Without diffusion, FSTAS shows significant variance for clips of different lengths, indicating unstable mapping to the label space. With diffusion, prediction feature correlations improve across different clip lengths, demonstrating that the diffusion structure enhances temporal generalization.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/output_feature2.jpg" alt="图片说明" width="600" align="center"></p>
<h3 id="comparison-to-prior-work">Comparison to prior work</h3>
<p>In Table FSTAS-DM is compared with existing TAS and STAS methods. STAS is more challenging than TAS due to limited model view and lack of context. FSTAS-DM achieves competitive results with SOTA TAS methods and outperforms existing STAS methods. On the Breakfast dataset, RGB quality is poorer than in Gtea and 50Salads, leading to suboptimal performance for RGB-only methods. While most TAS methods use multimodal inputs (RGB and optical flow) to compensate for poor RGB quality, FSTAS-DM follows the online STAS design, using only RGB input, resulting in lower performance on Breakfast.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/ctp.png" alt="图片说明" align="center"></p>
<h2 id="conclusion">Conclusion</h2>
<p>In this paper, we study VTDS using STAS as the foundation and propose FSTAS-DM as a solution. We integrate diffusion architecture into STAS, using streaming video clips with varying feature distributions as conditions for generating action label sequences, significantly improving temporal generalization. Additionally, the proposed MSCC further enhances the model's ability to learn complex distributions. Due to its strong temporal generalization, our method not only boosts performance but also lowers training costs when applying STAS to different video lengths.</p>
<h2 id="citation">Citation</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings{long2024noisy,</span><br><span class="line">  title={Flexible Streaming Temporal Action Segmentation with Diffusion Models},</span><br><span class="line">  author={Jinrong Zhang, Wenjun Wen, Shenglan Liu, Sifan Zhang, Yuning Ding and Lin Feng},</span><br><span class="line">  booktitle={2025 IEEE International Conference on Multimedia and Expo (ICME)},</span><br><span class="line">  year={2025},</span><br><span class="line">  organization={IEEE}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>papers</category>
      </categories>
  </entry>
  <entry>
    <title>papers</title>
    <url>/2025/03/25/papers/</url>
    <content><![CDATA[<figure>
<img src="/images/papers/zjr2.jpg" alt="图片" /><figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>您好您好</p>
]]></content>
      <categories>
        <category>papers</category>
      </categories>
  </entry>
</search>
