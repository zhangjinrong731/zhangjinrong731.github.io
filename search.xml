<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello, My name is Jinrong Zhang</title>
    <url>/2025/03/25/hello-world/</url>
    <content><![CDATA[<h1 id="helloits-a-happy-day">👋 Hello，it's a happy day~~~</h1>
<ul>
<li>My name is <strong>Jinrong Zhang</strong>, and I am a researcher specializing in computer vision and multimodal large models. 🚀</li>
<li>If you have any interest in collaboration or academic exchange, please feel free to contact me.</li>
</ul>
<hr>
<h3 id="about-me">🧑‍💻 About Me</h3>
<p>📚 <strong>PhD Student</strong> in Electronic Information at Harbin Institute of Technology, Shenzhen.<br>
🔬 <strong>Research Interests:</strong><br>
- Video Understanding and Generation \ - Multimodal Representation\ - Temporal Action Segmentation</p>
<hr>
<h3 id="research-papers">📄 Research Papers</h3>
<p>I love publishing and sharing my findings with the world! Here's a list of some of my published research papers:</p>
<ol type="1">
<li><p><strong>Just a Few Glances: Open-Set Visual Perception with Image Prompt Paradigm</strong> – AAAI, CCF-A, 2025<br>
🔗 <a href="https://arxiv.org/abs/2412.10719">Link to orginal paper</a></p></li>
<li><p><strong>End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning</strong> – TNNLS, CCF-B, IF=10.2, 2025<br>
🔗 <a href="https://arxiv.org/abs/2309.15683">Link to orginal paper</a></p></li>
<li><p><strong>Flexible Streaming Temporal Action Segmentation with Diffusion Models</strong> – ICME, CCF-B, 2025<br>
🔗 <a href="">Accepted (to be indexed soon)</a></p></li>
<li><p><strong>DTOS: Dynamic Time Object Sensing with Large Multimodal Model</strong> – CVPR, CCF-A, 2025<br>
🔗 <a href="">Accepted (to be indexed soon)</a></p></li>
<li><p><strong>Cluster-Refined Optimal Transport for Unsupervised Action Segmentation</strong> – ICASSP, CCF-B, 2025<br>
🔗 <a href="https://ieeexplore.ieee.org/abstract/document/10887693">Link to orginal paper</a></p></li>
</ol>
<p>On the Papers page, you can also access the key details of these research papers.</p>
<hr>
<h3 id="internship-experience">💼 Internship Experience</h3>
<ul>
<li><strong>Xiaomi AI Lab</strong> – <strong>AI Research Intern</strong><br>
<em>2024/2 – 2025/10</em>
<ul>
<li>I provided a large model solution for access permission detection at the Xiaomi car factory and successfully implemented it.</li>
<li>During my internship, I published a paper in AAAI.</li>
</ul></li>
</ul>
<hr>
<h3 id="my-profile">🌐 My Profile</h3>
<ul>
<li>🌍 Website: <a href="https://scholar.google.com/citations?user=doMwcRYAAAAJ&amp;hl=en">Google Shcoral</a></li>
</ul>
<hr>
]]></content>
  </entry>
  <entry>
    <title>Flexible Streaming Temporal Action Segmentation with Diffusion Models</title>
    <url>/2025/03/25/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/</url>
    <content><![CDATA[<h1 id="flexible-streaming-temporal-action-segmentation-with-diffusion-models">Flexible Streaming Temporal Action Segmentation with Diffusion Models</h1>
<p><strong>Jinrong Zhang</strong><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="0.898ex" height="1.949ex" role="img" focusable="false" viewBox="0 -861.5 397 861.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><path data-c="2020" d="M182 675Q195 705 222 705Q234 705 243 700T253 691T263 675L262 655Q262 620 252 549T240 454V449Q250 451 288 461T346 472T377 461T389 431Q389 417 379 404T346 390Q327 390 288 401T243 412H240V405Q245 367 250 339T258 301T261 274T263 225Q263 124 255 -41T239 -213Q236 -216 222 -216H217Q206 -216 204 -212T200 -186Q199 -175 199 -168Q181 38 181 225Q181 265 182 280T191 327T204 405V412H201Q196 412 157 401T98 390Q76 390 66 403T55 431T65 458T98 472Q116 472 155 462T205 449Q204 452 204 460T201 490T193 547Q182 619 182 655V675Z"></path></g></g></g></g></svg></mjx-container></span>, Wenjun Wen<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="0.898ex" height="1.949ex" role="img" focusable="false" viewBox="0 -861.5 397 861.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><path data-c="2020" d="M182 675Q195 705 222 705Q234 705 243 700T253 691T263 675L262 655Q262 620 252 549T240 454V449Q250 451 288 461T346 472T377 461T389 431Q389 417 379 404T346 390Q327 390 288 401T243 412H240V405Q245 367 250 339T258 301T261 274T263 225Q263 124 255 -41T239 -213Q236 -216 222 -216H217Q206 -216 204 -212T200 -186Q199 -175 199 -168Q181 38 181 225Q181 265 182 280T191 327T204 405V412H201Q196 412 157 401T98 390Q76 390 66 403T55 431T65 458T98 472Q116 472 155 462T205 449Q204 452 204 460T201 490T193 547Q182 619 182 655V675Z"></path></g></g></g></g></svg></mjx-container></span>, Shenglan Liu, Sifan Zhang, Yuning Ding and Lin Feng 🔗 <a href="">Accepted by ICME25 (to be indexed soon)</a></p>
<h2 id="abstract">Abstract</h2>
<p>Temporal distribution shifts occur not only in low-dimensional time-series data but also in high-dimensional data like videos. This phenomenon leads to significant performance degeneration in video understanding methods such as streaming temporal action segmentation. To address this issue, we propose a flexible streaming temporal action segmentation model with diffusion models (FSTAS-DM). By utilizing streaming video clips with varying feature distributions as control conditions, our model can adapt to the shifts and inconsistency of the distribution between the training and testing domains. Additionally, we have introduced a multi-stage conditional control training strategy (MSCC), which enhances the temporal generalization ability of the model. Our method demonstrates commendable performance on datasets like GTEA, 50Salads, and Breakfast.</p>
<span id="more"></span>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models\main_fig2.jpg" width="500" alt="图片说明" align="center"></p>
<h2 id="introduction">Introduction</h2>
<p>Temporal distribution shifts (TDS) occur widely in the field of times series, refering to the distribution shifts between training and testing domains. This inconsistency hinders achieving optimal performance. Most TDS studies focus on low-dimensional data, such as finance, weather, and electricity usage. However, TDS also occurs in high-dimensional data like videos. To facilitate training, video understanding models are always trained by a fixed number of frames that sample from videos in the whole training process. Those models perform well when tested with the same number of frames, but performance degradation occurred once the number of frames changed. We called this phenomenon video temporal distribution shifts (VTDS) because there are shifts and inconsistencies in video feature distributions between training and testing domains, as shown in Figure. VTDS severely limits the temporal generalization capability of video understanding models, which refers to accurately and stably mapping feature data with previously unseen distributions from feature space to label space. If a model is trained on video clips of the same length, it tends to be trained only on samples with a similar data distribution, significantly limiting the model's application scope.</p>
<p>Streaming temporal action segmentation (STAS) task, which decomposes a full video into a stream of video clips and classifies each frame of every clip in temporal order, is a task significantly affected by VTDS. In practical scenarios, different capture devices provide streaming video clips of varying lengths, and different applications have varying requirements. However, for ease of engineering implementation, current models use the same clip length for both training and testing. When the length of video clips used in testing does not align with the training, the performance of models is substantially decreased. It is VTDS that is consistent with the phenomenon in the STAS task.</p>
<p>To mitigate VTDS, we introduce the diffusion structure into STAS, inspired by DiffAct. Diffusion models, exceling in learning complex and diverse data distributions, are widely used in image generation, style transfer, and time-series tasks. Inspired by this, we propose FSTAS-DM, a flexible streaming temporal action segmentation model with diffusion models. Our model generates predicted labels from noisy data with RGB videos as control inputs. The diffusion structure enables learning diverse distributions and adapting to distribution shifts in the testing domain. We further enhance generalization with a multi-stage conditional control strategy (MSCC), dividing training into stages. In each stage, streaming clips with different feature distributions guide the diffusion process. This stage-by-stage learning helps denoise sequences into final temporal label predictions.</p>
<p>In summary, this paper presents three main contributions: (1) We discover the phenomenon of VTDS in the video understanding field and tackle it by proposing FSTAS-DM. (2) The multi-stage conditional control training strategy we proposed can enhance the temporal generalization ability and significantly reduce the training cost when STAS models are applied to different video clip lengths. (3) The proposed method achieves commendable performance on datatsets like GTEA, 50Salads, and Breakfast.</p>
<h2 id="method">Method</h2>
<p>FSTAS-DM consists of a streaming video conditional control process and an inference process with conditional control, as shown in Figure. During the streaming video conditional control process, conditional latent variables are obtained through a flexible prompt network under different conditional information. Streaming video clips with unique data distribution, namely, which have different clip lengths, are used as diverse conditional information. In the inference process with conditional control, a conditional control denoising model (CCDM), guided by the conditional latent variables, denoises the original Gaussian noise, to produce the next stage result. Iteratively applying this inference process ultimately yields the predicted segmentation result of the streaming video clip.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models\Model5.jpg" alt="图片说明" align="center"></p>
<h2 id="experiments">Experiments</h2>
<h3 id="the-impact-of-video-temporal-distribution-shifts">The Impact of Video Temporal Distribution Shifts</h3>
<p>Figure illustrates the significant impact of VTDS on model performance. We adopt three typical temporal action segmentation models with entirely different structures to STAS, including MSTCN, ASFormer, and DiffAct. We train them on video clip streams with a fixed length of 128 and test them across a variety of clip lengths. It is apparent that VTDS leads to a severe training-testing discrepancy, which means model performance deteriorates as the difference in video clip lengths between training and testing increases. Such performance degradation highlights the inadequate temporal generalization capability of existing temporal action segmentation methods, showing their difficulty in adapting to the altered data distribution caused by VTDS.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/Acc_F054.jpg" alt="图片说明" width="500" align="center"></p>
<p>We also compare FSTAS-DM with these three methods. It is evident that FSTAS-DM not only significantly outperforms these three methods in STAS but also greatly mitigates the impact of VTDS.</p>
<h3 id="the-temporal-generalization-capability-of-fstas-dm">The Temporal Generalization Capability of FSTAS-DM</h3>
<p>Figure shows FSTAS-DM's temporal generalization capability. Trained and tested on streaming clips of varying lengths, it outperformed fixed-length models, which struggled with shorter clips. Using MSCC with clip lengths of 32, 64, and 128 improved performance across all lengths. The results demonstrate that FSTAS-DM captures VTDS's impact on data distribution by learning from limited samples.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/flexible_gtea7.jpg" alt="图片说明" align="center"></p>
<p>As mentioned, we introduce the diffusion structure into STAS to enhance temporal generalization. Figure  shows its impact on streaming video features. STAS aims to map streaming video clips from an undefined data space to a relatively stable label space. To analyze the effect of diffusion on VTDS, we removed the diffusion structure from FSTAS-DM, creating FSTAS. Figure compares the cosine similarity matrices of prediction features for FSTAS-DM and FSTAS. Without diffusion, FSTAS shows significant variance for clips of different lengths, indicating unstable mapping to the label space. With diffusion, prediction feature correlations improve across different clip lengths, demonstrating that the diffusion structure enhances temporal generalization.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/output_feature2.jpg" alt="图片说明" width="600" align="center"></p>
<h3 id="comparison-to-prior-work">Comparison to prior work</h3>
<p>In Table FSTAS-DM is compared with existing TAS and STAS methods. STAS is more challenging than TAS due to limited model view and lack of context. FSTAS-DM achieves competitive results with SOTA TAS methods and outperforms existing STAS methods. On the Breakfast dataset, RGB quality is poorer than in Gtea and 50Salads, leading to suboptimal performance for RGB-only methods. While most TAS methods use multimodal inputs (RGB and optical flow) to compensate for poor RGB quality, FSTAS-DM follows the online STAS design, using only RGB input, resulting in lower performance on Breakfast.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/ctp.png" alt="图片说明" align="center"></p>
<h2 id="conclusion">Conclusion</h2>
<p>In this paper, we study VTDS using STAS as the foundation and propose FSTAS-DM as a solution. We integrate diffusion architecture into STAS, using streaming video clips with varying feature distributions as conditions for generating action label sequences, significantly improving temporal generalization. Additionally, the proposed MSCC further enhances the model's ability to learn complex distributions. Due to its strong temporal generalization, our method not only boosts performance but also lowers training costs when applying STAS to different video lengths.</p>
<h2 id="citation">Citation</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings{long2024noisy,</span><br><span class="line">  title={Flexible Streaming Temporal Action Segmentation with Diffusion Models},</span><br><span class="line">  author={Jinrong Zhang, Wenjun Wen, Shenglan Liu, Sifan Zhang, Yuning Ding and Lin Feng},</span><br><span class="line">  booktitle={2025 IEEE International Conference on Multimedia and Expo (ICME)},</span><br><span class="line">  year={2025},</span><br><span class="line">  organization={IEEE}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>papers</category>
      </categories>
  </entry>
  <entry>
    <title>papers</title>
    <url>/2025/03/25/papers/</url>
    <content><![CDATA[<figure>
<img src="/images/papers/zjr2.jpg" alt="图片" /><figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>您好您好</p>
]]></content>
      <categories>
        <category>papers</category>
      </categories>
  </entry>
</search>
