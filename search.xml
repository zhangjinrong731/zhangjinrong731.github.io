<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello, My name is Jinrong Zhang</title>
    <url>/2025/03/25/hello-world/</url>
    <content><![CDATA[<h1 id="helloits-a-happy-day">👋 Hello，it's a happy day~~~</h1>
<ul>
<li>My name is <strong>Jinrong Zhang</strong>, and I am a researcher specializing in computer vision and multimodal large models. 🚀</li>
<li>If you have any interest in collaboration or academic exchange, please feel free to contact me.</li>
</ul>
<hr>
<h3 id="about-me">🧑‍💻 About Me</h3>
<p>📚 <strong>PhD Student</strong> in Electronic Information at Harbin Institute of Technology, Shenzhen.<br>
🔬 <strong>Research Interests:</strong></p>
<ul>
<li>Video Understanding and Generation</li>
<li>Multimodal Representation</li>
<li>Temporal Action Segmentation</li>
</ul>
<hr>
<h3 id="research-papers">📄 Research Papers</h3>
<p>I love publishing and sharing my findings with the world! Here's a list of some of my published research papers:</p>
<ol type="1">
<li><p><strong>Just a Few Glances: Open-Set Visual Perception with Image Prompt Paradigm</strong> – AAAI, CCF-A, 2025</p>
<p>🔗 <a href="https://arxiv.org/abs/2412.10719">Link to orginal paper</a></p></li>
<li><p><strong>End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning</strong> – TNNLS, CCF-B, IF=10.2, 2025</p>
<p>🔗 <a href="https://arxiv.org/abs/2309.15683">Link to orginal paper</a></p></li>
<li><p><strong>Flexible Streaming Temporal Action Segmentation with Diffusion Models</strong> – ICME, CCF-B, 2025</p>
<p>🔗 <a href="">Accepted (to be indexed soon)</a></p></li>
<li><p><strong>DTOS: Dynamic Time Object Sensing with Large Multimodal Model</strong> – CVPR, CCF-A, 2025</p>
<p>🔗 <a href="">Accepted (to be indexed soon)</a></p></li>
<li><p><strong>Cluster-Refined Optimal Transport for Unsupervised Action Segmentation</strong> – ICASSP, CCF-B, 2025</p>
<p>🔗 <a href="https://ieeexplore.ieee.org/abstract/document/10887693">Link to orginal paper</a></p></li>
</ol>
<p>On the Papers page, you can also access the key details of these research papers.</p>
<hr>
<h3 id="internship-experience">💼 Internship Experience</h3>
<ul>
<li><strong>Xiaomi AI Lab</strong> – <strong>AI Research Intern</strong><br>
<em>2024/2 – 2025/10</em>
<ul>
<li>I provided a large model solution for access permission detection at the Xiaomi car factory and successfully implemented it.</li>
<li>During my internship, I published a paper in AAAI.</li>
</ul></li>
</ul>
<hr>
<h3 id="my-profile">🌐 My Profile</h3>
<ul>
<li>🌍 Website: <a href="https://scholar.google.com/citations?user=doMwcRYAAAAJ&amp;hl=en">Google Shcoral</a></li>
</ul>
<hr>
]]></content>
  </entry>
  <entry>
    <title>Flexible Streaming Temporal Action Segmentation with Diffusion Models</title>
    <url>/2025/03/25/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/</url>
    <content><![CDATA[<h1 id="flexible-streaming-temporal-action-segmentation-with-diffusion-models">Flexible Streaming Temporal Action Segmentation with Diffusion Models</h1>
<p><strong>Jinrong Zhang</strong><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="0.898ex" height="1.949ex" role="img" focusable="false" viewBox="0 -861.5 397 861.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><path data-c="2020" d="M182 675Q195 705 222 705Q234 705 243 700T253 691T263 675L262 655Q262 620 252 549T240 454V449Q250 451 288 461T346 472T377 461T389 431Q389 417 379 404T346 390Q327 390 288 401T243 412H240V405Q245 367 250 339T258 301T261 274T263 225Q263 124 255 -41T239 -213Q236 -216 222 -216H217Q206 -216 204 -212T200 -186Q199 -175 199 -168Q181 38 181 225Q181 265 182 280T191 327T204 405V412H201Q196 412 157 401T98 390Q76 390 66 403T55 431T65 458T98 472Q116 472 155 462T205 449Q204 452 204 460T201 490T193 547Q182 619 182 655V675Z"></path></g></g></g></g></svg></mjx-container></span>, Wenjun Wen<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="0.898ex" height="1.949ex" role="img" focusable="false" viewBox="0 -861.5 397 861.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><path data-c="2020" d="M182 675Q195 705 222 705Q234 705 243 700T253 691T263 675L262 655Q262 620 252 549T240 454V449Q250 451 288 461T346 472T377 461T389 431Q389 417 379 404T346 390Q327 390 288 401T243 412H240V405Q245 367 250 339T258 301T261 274T263 225Q263 124 255 -41T239 -213Q236 -216 222 -216H217Q206 -216 204 -212T200 -186Q199 -175 199 -168Q181 38 181 225Q181 265 182 280T191 327T204 405V412H201Q196 412 157 401T98 390Q76 390 66 403T55 431T65 458T98 472Q116 472 155 462T205 449Q204 452 204 460T201 490T193 547Q182 619 182 655V675Z"></path></g></g></g></g></svg></mjx-container></span>, Shenglan Liu, Sifan Zhang, Yuning Ding and Lin Feng 🔗 <a href="">Accepted by ICME25 (to be indexed soon)</a></p>
<h2 id="abstract">Abstract</h2>
<p>Temporal distribution shifts occur not only in low-dimensional time-series data but also in high-dimensional data like videos. This phenomenon leads to significant performance degeneration in video understanding methods such as streaming temporal action segmentation. To address this issue, we propose a flexible streaming temporal action segmentation model with diffusion models (FSTAS-DM). By utilizing streaming video clips with varying feature distributions as control conditions, our model can adapt to the shifts and inconsistency of the distribution between the training and testing domains. Additionally, we have introduced a multi-stage conditional control training strategy (MSCC), which enhances the temporal generalization ability of the model. Our method demonstrates commendable performance on datasets like GTEA, 50Salads, and Breakfast.</p>
<span id="more"></span>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models\main_fig2.jpg" width="500" alt="图片说明" align="center"></p>
<h2 id="introduction">Introduction</h2>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.791ex;" xmlns="http://www.w3.org/2000/svg" width="17.956ex" height="2.722ex" role="img" focusable="false" viewBox="0 -853.7 7936.7 1203.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(714,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1348,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mrow" transform="translate(2292.7,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7B" d="M477 -343L471 -349H458Q432 -349 367 -325T273 -263Q258 -245 250 -212L249 -51Q249 -27 249 12Q248 118 244 128Q243 129 243 130Q220 189 121 228Q109 232 107 235T105 250Q105 256 105 257T105 261T107 265T111 268T118 272T128 276T142 283T162 291Q224 324 243 371Q243 372 244 373Q248 384 249 469Q249 475 249 489Q249 528 249 552L250 714Q253 728 256 736T271 761T299 789T347 816T422 843Q440 849 441 849H443Q445 849 447 849T452 850T457 850H471L477 844V830Q477 820 476 817T470 811T459 807T437 801T404 785Q353 760 338 724Q333 710 333 550Q333 526 333 492T334 447Q334 393 327 368T295 318Q257 280 181 255L169 251L184 245Q318 198 332 112Q333 106 333 -49Q333 -209 338 -223Q351 -255 391 -277T469 -309Q477 -311 477 -329V-343Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(583,0)"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(714,363) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(714,-247) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g><g data-mml-node="mo" transform="translate(1117.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1562.2,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(2900.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(3345.6,0)"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(714,363) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mi" transform="translate(714,-247) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5061,0) translate(0 -0.5)"><path data-c="7D" d="M110 849L115 850Q120 850 125 850Q151 850 215 826T309 764Q324 747 332 714L333 552Q333 528 333 489Q334 383 338 373Q339 372 339 371Q353 336 391 310T469 271Q477 268 477 251Q477 241 476 237T472 232T456 225T428 214Q357 179 339 130Q339 129 338 128Q334 117 333 32Q333 26 333 12Q333 -27 333 -51L332 -212Q328 -228 323 -240T302 -271T255 -307T175 -338Q139 -349 125 -349T108 -346T105 -329Q105 -314 107 -312T130 -304Q233 -271 248 -209Q249 -203 249 -49V57Q249 106 253 125T273 167Q307 213 398 245L413 251L401 255Q265 300 250 389Q249 395 249 550Q249 710 244 724Q224 774 112 811Q105 813 105 830Q105 845 110 849Z"></path></g></g></g></g></svg></mjx-container></span></p>
<p>Temporal distribution shifts (TDS) occur widely in the field of times series, refering to the distribution shifts between training and testing domains. This inconsistency hinders achieving optimal performance. Most TDS studies focus on low-dimensional data, such as finance, weather, and electricity usage. However, TDS also occurs in high-dimensional data like videos. To facilitate training, video understanding models are always trained by a fixed number of frames that sample from videos in the whole training process. Those models perform well when tested with the same number of frames, but performance degradation occurred once the number of frames changed. We called this phenomenon video temporal distribution shifts (VTDS) because there are shifts and inconsistencies in video feature distributions between training and testing domains, as shown in Figure. VTDS severely limits the temporal generalization capability of video understanding models, which refers to accurately and stably mapping feature data with previously unseen distributions from feature space to label space. If a model is trained on video clips of the same length, it tends to be trained only on samples with a similar data distribution, significantly limiting the model's application scope.</p>
<p>Streaming temporal action segmentation (STAS) task, which decomposes a full video into a stream of video clips and classifies each frame of every clip in temporal order, is a task significantly affected by VTDS. In practical scenarios, different capture devices provide streaming video clips of varying lengths, and different applications have varying requirements. However, for ease of engineering implementation, current models use the same clip length for both training and testing. When the length of video clips used in testing does not align with the training, the performance of models is substantially decreased. It is VTDS that is consistent with the phenomenon in the STAS task.</p>
<p>To mitigate VTDS, we introduce the diffusion structure into STAS, inspired by DiffAct. Diffusion models, exceling in learning complex and diverse data distributions, are widely used in image generation, style transfer, and time-series tasks. Inspired by this, we propose FSTAS-DM, a flexible streaming temporal action segmentation model with diffusion models. Our model generates predicted labels from noisy data with RGB videos as control inputs. The diffusion structure enables learning diverse distributions and adapting to distribution shifts in the testing domain. We further enhance generalization with a multi-stage conditional control strategy (MSCC), dividing training into stages. In each stage, streaming clips with different feature distributions guide the diffusion process. This stage-by-stage learning helps denoise sequences into final temporal label predictions.</p>
<p>In summary, this paper presents three main contributions: (1) We discover the phenomenon of VTDS in the video understanding field and tackle it by proposing FSTAS-DM. (2) The multi-stage conditional control training strategy we proposed can enhance the temporal generalization ability and significantly reduce the training cost when STAS models are applied to different video clip lengths. (3) The proposed method achieves commendable performance on datatsets like GTEA, 50Salads, and Breakfast.</p>
<h2 id="method">Method</h2>
<p>FSTAS-DM consists of a streaming video conditional control process and an inference process with conditional control, as shown in Figure. During the streaming video conditional control process, conditional latent variables are obtained through a flexible prompt network under different conditional information. Streaming video clips with unique data distribution, namely, which have different clip lengths, are used as diverse conditional information. In the inference process with conditional control, a conditional control denoising model (CCDM), guided by the conditional latent variables, denoises the original Gaussian noise, to produce the next stage result. Iteratively applying this inference process ultimately yields the predicted segmentation result of the streaming video clip.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models\Model5.jpg" alt="图片说明" align="center"></p>
<h2 id="experiments">Experiments</h2>
<h3 id="the-impact-of-video-temporal-distribution-shifts">The Impact of Video Temporal Distribution Shifts</h3>
<p>Figure illustrates the significant impact of VTDS on model performance. We adopt three typical temporal action segmentation models with entirely different structures to STAS, including MSTCN, ASFormer, and DiffAct. We train them on video clip streams with a fixed length of 128 and test them across a variety of clip lengths. It is apparent that VTDS leads to a severe training-testing discrepancy, which means model performance deteriorates as the difference in video clip lengths between training and testing increases. Such performance degradation highlights the inadequate temporal generalization capability of existing temporal action segmentation methods, showing their difficulty in adapting to the altered data distribution caused by VTDS.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/Acc_F054.jpg" alt="图片说明" width="500" align="center"></p>
<p>We also compare FSTAS-DM with these three methods. It is evident that FSTAS-DM not only significantly outperforms these three methods in STAS but also greatly mitigates the impact of VTDS.</p>
<h3 id="the-temporal-generalization-capability-of-fstas-dm">The Temporal Generalization Capability of FSTAS-DM</h3>
<p>Figure shows FSTAS-DM's temporal generalization capability. Trained and tested on streaming clips of varying lengths, it outperformed fixed-length models, which struggled with shorter clips. Using MSCC with clip lengths of 32, 64, and 128 improved performance across all lengths. The results demonstrate that FSTAS-DM captures VTDS's impact on data distribution by learning from limited samples.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/flexible_gtea7.jpg" alt="图片说明" align="center"></p>
<p>As mentioned, we introduce the diffusion structure into STAS to enhance temporal generalization. Figure  shows its impact on streaming video features. STAS aims to map streaming video clips from an undefined data space to a relatively stable label space. To analyze the effect of diffusion on VTDS, we removed the diffusion structure from FSTAS-DM, creating FSTAS. Figure compares the cosine similarity matrices of prediction features for FSTAS-DM and FSTAS. Without diffusion, FSTAS shows significant variance for clips of different lengths, indicating unstable mapping to the label space. With diffusion, prediction feature correlations improve across different clip lengths, demonstrating that the diffusion structure enhances temporal generalization.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/output_feature2.jpg" alt="图片说明" width="600" align="center"></p>
<h3 id="comparison-to-prior-work">Comparison to prior work</h3>
<p>In Table FSTAS-DM is compared with existing TAS and STAS methods. STAS is more challenging than TAS due to limited model view and lack of context. FSTAS-DM achieves competitive results with SOTA TAS methods and outperforms existing STAS methods. On the Breakfast dataset, RGB quality is poorer than in Gtea and 50Salads, leading to suboptimal performance for RGB-only methods. While most TAS methods use multimodal inputs (RGB and optical flow) to compensate for poor RGB quality, FSTAS-DM follows the online STAS design, using only RGB input, resulting in lower performance on Breakfast.</p>
<p><img src="/images/Flexible-Streaming-Temporal-Action-Segmentation-with-Diffusion-Models/ctp.png" alt="图片说明" align="center"></p>
<h2 id="conclusion">Conclusion</h2>
<p>In this paper, we study VTDS using STAS as the foundation and propose FSTAS-DM as a solution. We integrate diffusion architecture into STAS, using streaming video clips with varying feature distributions as conditions for generating action label sequences, significantly improving temporal generalization. Additionally, the proposed MSCC further enhances the model's ability to learn complex distributions. Due to its strong temporal generalization, our method not only boosts performance but also lowers training costs when applying STAS to different video lengths.</p>
<h2 id="citation">Citation</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings{long2024noisy,</span><br><span class="line">  title={Flexible Streaming Temporal Action Segmentation with Diffusion Models},</span><br><span class="line">  author={Jinrong Zhang, Wenjun Wen, Shenglan Liu, Sifan Zhang, Yuning Ding and Lin Feng},</span><br><span class="line">  booktitle={2025 IEEE International Conference on Multimedia and Expo (ICME)},</span><br><span class="line">  year={2025},</span><br><span class="line">  organization={IEEE}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>papers</category>
      </categories>
  </entry>
  <entry>
    <title>papers</title>
    <url>/2025/03/25/papers/</url>
    <content><![CDATA[<figure>
<img src="/images/papers/zjr2.jpg" alt="图片" /><figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>您好您好</p>
]]></content>
      <categories>
        <category>papers</category>
      </categories>
  </entry>
</search>
